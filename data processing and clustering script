#Step 1: Import Necessary Modules
import sys
import os.path
import pandas as pd
import numpy as np
from numpy import where
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.cluster import KMeans
#from sklearn.datasets.samples_generator import make_blobs
from sklearn.preprocessing import StandardScaler

np.random.seed(0)
centers = [[1, 1], [-1, -1]]
n_clusters = len(centers)
#X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.5)

file_path = 'output23_file.txt'
sys.stdout = open(file_path, "w")


#Step 2: Import csv file
df = pd.read_csv("Satellites_reading_update_23.csv", sep=',', header=0)
#view first five rows of DataFrame
#print(df.head())

df.shape

#print(df.info())

df.isnull().sum()


#Step 3: Clean & Prep the DataFrame
#drop rows with NA values in any columns
df = df.dropna()

#create scaled DataFrame where each variable has mean of 0 and standard dev of 1
scaled_df = StandardScaler().fit_transform(df)

#view first five rows of scaled DataFrame
#print(scaled_df[:5])

# Randomly split the dataset into train data (70% of dataset) and test data (30% of dataset)
rng = RandomState()
train = df.sample(frac=0.7, random_state=rng)
test = df.loc[~df.index.isin(train.index)]


#Step 4: Find the Optimal Number of Clusters
#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": 1,
}

#create list to hold SSE values for each k
sse = [] #sum of squared errors
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_df)
    sse.append(kmeans.inertia_)

#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
#plt.show()
plt.savefig('Cluster23_graph.png')
#Note: Where the sum of squares begins to “bend” or level off, this is typically the optimal number of clusters.

#Step 5: Perform K-Means Clustering with Optimal K
#instantiate the k-means class, using optimal number of clusters
kmeans = KMeans(init="random", n_clusters=2, n_init=10, random_state=1)

#fit k-means algorithm to data
kmeans.fit(scaled_df)

#append cluster assingments to original DataFrame
df['cluster'] = kmeans.labels_
#view updated DataFrame
print(df.to_string())
#print(df, file = output_file)

test_mean = df.mean()
print(test_mean, "Mean")
test_std = df.std()
print(test_std, "Standard Deviation")
range0 = test_mean - test_std
print(range0)
range1 = test_mean + test_std
print(range1)


def GS_Pro_Cluster(value):
    if range0['GS_PROCESSOR_TEMPERATURE'] <= value <= range1['GS_PROCESSOR_TEMPERATURE']:
        return 1
    return 0
df['GS_Pro_Cluster'] = df['GS_PROCESSOR_TEMPERATURE'].map(GS_Pro_Cluster)


def Com_Pro_cluster(value):
    if range0['COM_PROCESSOR_VOLTAGE'] <= value <= range1['COM_PROCESSOR_VOLTAGE']:
        return 1
    return 0
df['Com_Pro_cluster'] = df['COM_PROCESSOR_VOLTAGE'].map(Com_Pro_cluster)


def GS_Voltage_cluster(value):
    if range0['GS_PROCESSOR_VOLTAGE'] <= value <= range1['GS_PROCESSOR_VOLTAGE']:
        return 1
    return 0
df['GS_Voltage_cluster'] = df['GS_PROCESSOR_VOLTAGE'].map(GS_Voltage_cluster)


def Com_Voltage_cluster(value):
    if range0['COM_PROCESSOR_VOLTAGE'] <= value <= range1['COM_PROCESSOR_VOLTAGE']:
        return 1
    return 0
df['Com_Voltage_cluster'] = df['COM_PROCESSOR_VOLTAGE'].map(Com_Voltage_cluster)


def RotationSpeed_cluster(value):
    if range0['ADCS_ROTATIONAL_SPEED'] <= value <= range1['ADCS_ROTATIONAL_SPEED']:
        return 1
    return 0
df['RotationSpeed_cluster'] = df['ADCS_ROTATIONAL_SPEED'].map(RotationSpeed_cluster)


def MagneticAngle_cluster(value):
    if range0['ADCS_MAGNETIC_ANGLE'] <= value <= range1['ADCS_MAGNETIC_ANGLE']:
        return 1
    return 0
df['MagneticAngle_cluster'] = df['ADCS_MAGNETIC_ANGLE'].map(MagneticAngle_cluster)


col1_list = df[["GS_Pro_cluster", "Com_Pro_cluster", "GS_Voltage_cluster", "Com_Voltage_cluster", "RotationSpeed_cluster", "MagneticAngle_cluster", "cluster"]]
#print(col1_list)

col_list = col1_list.sort_values(by=['cluster'])
#print(col_list)
result = col_list.loc[df['cluster'] == 1]
#print(result)
df_list = result.values.tolist()
df_lists = ';'.join(map(str, df_list))
#print(df_lists)

result_1 = str(df_lists)
result_2 = result_1.replace("[", "")
result_3 = result_2.replace("]", "")
df_list1 = result_3.replace(" ", '')
#print(df_list1)
#---------------------------------
result1 = col_list.loc[df['cluster'] == 0]
#print(result1)
df_list01 = result1.values.tolist()
df_list02 = ';'.join(map(str, df_list01))
#print(df_lists)
result_01 = str(df_list02)
result_02 = result_01.replace("[", "")
result_03 = result_02.replace("]", "")
df_list2 = result_03.replace(" ", '')

print(df_list1, df_list2, sep = '\n---\n')


#----------Phase 3: LTL Learning Data Prepapration--------------------------------------------
#Step 3.1 Locate the time series of bad state(s)

#Step 3.2 Extracts n consecutive instances before the bad state(s)
#Filter & Get n (-10) rows before and n (+10) after specific value (time series) in pandas
result = df.loc[df['cluster'] == 0]

result1 = df.loc[df['cluster'] == 1]

#result2 = df.loc[:, ['GS_Pro_Cluster', 'Com_Pro_Cluster', 'GS_Voltage_Cluster', 'Com_Voltage_Cluster', 'GS_RotationSpeed_Cluster', 'MagneticAngle_Cluster', 'cluster'] == 0].index
#print(result2)

#print(result)
#print(result1)

#Get the index values and then get the previous two row index values:
idx = df.loc[df['cluster'] == 0].index
#print(idx, 'what is this')

filtered_idx = (idx-1).union(idx-10)
filtered_idx = filtered_idx[filtered_idx > 0]

df_new = df.iloc[filtered_idx]
print(df_new, 'what is this')


#print()
#star = "-----------------------------------------------------------------------------------------------------------------------------------------"
#print(df_new)

#print(df_new,  file= trace_file)

print()
#star = "-----------------------------------------------------------------------------------------------------------------------------------------"
#print(star, file= trace_file)
df1 = df_new["cluster"].tolist()
df1 = ';'.join(map(str, df1))
print(df1, file= file_name)
#print(df1, sep=',')



# Generate the LTL formular

#Save traces into example.trace &&

#/Desktop/Application/runtime/venv/lib/python3.10/site-packages$ python3 -m Scarlet.ltllearner

#import os.path


